---
title: 'Group 3 - Chapter 9: Support Vector Machines Tutorial'
author:
- Jolien Lam (j.lam@amc.uva.nl) – University of Amsterdam
- Yeng Moua (moua0199@umn.edu) – University of Minnesota
- Fortuna Vuthaj (fortuna.vuthaj@gmail.com) – University of Heilbronn/Heidelberg
- Ronald Buie (buierw@uw.edu) – University of Washington
- Jessica Kuo (U101003014@cmu.edu.tw) – Taipei Medical University
- Diane Walker (diane.walker@utah.edu) – University of Utah
output:
  html_document:
    df_print: paged
---

```{r setup, include= FALSE}

`%!in%` = Negate(`%in%`)

memory.limit(200000)

library(dplyr) #load functions for transforming 
library(e1071)
library(ROCR)
```

This notebook documents work by group 3 utilizing support vector machines in the analysis of health data. Informaiton about the project can be found [here](https://iphie2018.wordpress.com/academic-program/).

# Front Matter

# Extraction

Data were acquired from ....

Two files are retrievable. One of the aggregate patient data, and one of mappings to descriptions of numeric values.

This later file was manually transformed into a long version for reading into our analysis environment.

```{r load_data}
PatientData <- read.csv("./Data/diabetic_data.csv", na.strings = "?")
#PatientData <- read.csv("./Data/20062018_cleanest_dataset_v2.csv", na.strings= "?")

Mappings <- read.csv("./Data/IDs_mapping_long.csv")

```

# Implimenting Exclusions and Variable Removals

We initially decided to match our data to that of the original researchers. The researchers excluded the variables "weight" and "payer code" due to missingness and, in the case of payer code, an expectation of no association. They kept "meedical specialty" but created an indicator of "missing" to account for ht ehigh proportion (47%) of missing values. Additionally, only the first visit (defined by the lowest encounter id per patient) was kept, patients discharged as deceased or to hospice were excluded. The researchers reported 69984 observations after all exlcusion, but the public data set contained 101766. The below code attempts to match the researcher's count.

## Removing Weight and and payer Code Variables

```{r remove_unused_variables}
PatientData$weight <- NULL
PatientData$payer_code <- NULL
```

## Excluding Hospice and Expired Patients

```{r excuding_hospice_expired}
#Excluding Dispositions of Hospice and Expired
PatientData <- PatientData[PatientData$discharge_disposition_id %!in% c(13,14,11,19,20,21),]
```

Discharge of hospice (discharge ids 13 and 14) and Deceased (discharge ids 11,19, 20, 21) were excluded from our data, leaving `r nrow(PatientData)`

## Excluding Visits Beyond the First

```{r}
PatientData <- PatientData %>% group_by(patient_nbr) %>% filter(row_number(encounter_id) == 1)
```

Limiting patients to the first observation (and discarding all others) brings our data to `r nrow(PatientData)` of an expected 69984.

## parameterizing Outcome

SVM require a Y of two levels, or special procedures to compensate. In this case our source study was conducted with readmissions under 30 days counted as a readmissions, and visits without readmissions or admissions after 30 days were counted as not having a readmission.

```{r}

levels(PatientData$readmitted) <- c("YES","NO","NO")

```

# Limiting Data

In order to be meaningufully used in our SVM, variables must have at least 2 values among the observations available. Variables with less were removed from the data set

```{r}
PatientData$acetohexamide <- NULL
PatientData$examide <- NULL
PatientData$citoglipton <- NULL
PatientData$glimepiride.pioglitazone <- NULL

summary(PatientData)
```

# Implementing a Support Vector Classifier

We began our exploration by implimenting a SVM with default options. We use 20% of our data as a training set and the remaining 80 as a test.

## Establishing a set of training and testing subsets

We define 3 sets of training and test subsets, each using a random 20/80 split of our data


```{r}
set.seed(1000)
trainingRows1 <- sample(1:nrow(PatientData), round(nrow(PatientData)*0.2,0))
trainingRows1 <- trainingRows1[order(trainingRows1)]
testingRows1 <- 1:nrow(PatientData) %!in% trainingRows1

set.seed(2000)
trainingRows2 <- sample(nrow(PatientData), round(nrow(PatientData)*0.2,0))
testingRows2 <- 1:nrow(PatientData) %!in% trainingRows2

set.seed(3000)
trainingRows3 <- sample(nrow(PatientData), round(nrow(PatientData)*0.2,0))
testingRows3 <- 1:nrow(PatientData) %!in% trainingRows3

```


## Tuning to identify the most efficient cost parameter

The tuning function identifies models with best performance on iterative runs against rand samples of the data (by default 10x cross validation), generating error and dispersion rates for each against the provided training data. It can perform this test for a series of parameterizations of the algorithm, in this case against multiple cost values, allowing us to identify the best performing cost to select.

```{r eval=FALSE, include=FALSE}

tunningValues <- tune(svm, readmitted ~ ., data = PatientData[trainingRows1,], kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tunningValues)
plot(tunningValues$best.model)
```

Using the tuning function we identify a cost of `r tuningValues$best.performance` as minimizing the number of support vectors while classifying t

```{r}
testData <- PatientData[testingRows1,]
set1FIT <- svm(readmitted ~ ., data = PatientData[trainingRows1,], kernel = "linear", cost = 10, scale = FALSE)
#summary(set1FIT)
Set1Prediction <- predict(set1FIT, testData, na.action = na.exclude)
linear1test <- table(predict = Set1Prediction[!is.na(Set1Prediction)], truth = as.data.frame(testData[!is.na(Set1Prediction),"readmitted"])$readmitted)

testData <- PatientData[testingRows2,]
set2FIT <- svm(readmitted ~ ., data = PatientData[trainingRows2,], kernel = "linear", cost = 10, scale = FALSE)
#summary(set2FIT)
Set2Prediction <- predict(set2FIT, testData, na.action = na.exclude)
linear2test <- table(predict = Set2Prediction[!is.na(Set2Prediction)], truth = as.data.frame(testData[!is.na(Set2Prediction),"readmitted"])$readmitted)

testData <- PatientData[testingRows3,]
set3FIT <- svm(readmitted ~ ., data = PatientData[trainingRows3,], kernel = "linear", cost = 10, scale = FALSE)
#summary(set3FIT)
Set3Prediction <- predict(set3FIT, testData, na.action = na.exclude)
linear3test <- table(predict = Set3Prediction[!is.na(Set3Prediction)], truth = as.data.frame(testData[!is.na(Set3Prediction),"readmitted"])$readmitted)
```

```{r}

set1FIT <- svm(readmitted ~ ., data = PatientData[trainingRows1,], kernel = "radial", cost = 10, gamma = 1, scale = FALSE)
summary(set1FIT)
testData <- PatientData[testingRows1,]
Set1Prediction <- predict(set1FIT, testData, na.action = na.exclude)
radial1test <- table(predict = Set1Prediction[!is.na(Set1Prediction)], truth = as.data.frame(testData[!is.na(Set1Prediction),"readmitted"])$readmitted)


set2FIT <- svm(readmitted ~ ., data = testData, kernel = "radial", cost = 10, gamma = 1, scale = FALSE)
summary(set2FIT)
testData <- PatientData[testingRows2,]
Set2Prediction <- predict(set2FIT, PatientData[trainingRows2,], na.action = na.exclude)
radial2test<- table(predict = Set2Prediction[!is.na(Set2Prediction)], truth = as.data.frame(testData[!is.na(Set2Prediction),"readmitted"])$readmitted)


set3FIT <- svm(readmitted ~ ., data = testData, kernel = "radial", cost = 10, gamma = 1, scale = FALSE)
summary(set3FIT)
testData <- PatientData[testingRows3,]
Set3Prediction <- predict(set3FIT, PatientData[trainingRows3,], na.action = na.exclude)
radial3test <- table(predict = Set3Prediction[!is.na(Set3Prediction)], truth = as.data.frame(testData[!is.na(Set3Prediction),"readmitted"])$readmitted)

```

# Plot ROC curves


# testing Caret library

```{r}
set.seed(3033)
intrain <- createDataPartition(y = PatientData$readmitted, p= 0.8, list = FALSE)
training <- PatientData[intrain,]
testing <- PatientData[-intrain,]

dim(training); dim(testing);


trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(3233)

svm_Linear <- train(readmitted ~., data = training, method = "svmLinear",
                    trControl=trctrl,
                    preProcess = c("center", "scale"),
                    tuneLength = 10)

svm_Linear


Pre-processing: centered (13), scaled (13) 
Resampling: Cross-Validated (10 fold, repeated 3 times) 
Summary of sample sizes: 189, 189, 189, 189, 189, 189, ... 
Resampling results:
  
  Accuracy  Kappa  
0.815873  0.62942

Tuning parameter 'C' was held constant at a value of 1

> test_pred <- predict(svm_Linear, newdata = testing)
> test_pred
[1] 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0 1 0
[45] 0 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0
[89] 1 0
Levels: 0 1

> confusionMatrix(test_pred, testing$V14 )
Confusion Matrix and Statistics

Reference
Prediction  0  1
0 45  5
1  7 33

Accuracy : 0.8667          
95% CI : (0.7787, 0.9292)
No Information Rate : 0.5778          
P-Value [Acc > NIR] : 2.884e-09       

Kappa : 0.7286          
Mcnemar's Test P-Value : 0.7728          

Sensitivity : 0.8654          
Specificity : 0.8684          
Pos Pred Value : 0.9000          
Neg Pred Value : 0.8250          
Prevalence : 0.5778          
Detection Rate : 0.5000          
Detection Prevalence : 0.5556          
Balanced Accuracy : 0.8669          

'Positive' Class : 0               

> grid <- expand.grid(C = c(0,0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2,5))
> set.seed(3233)
> svm_Linear_Grid <- train(V14 ~., data = training, method = "svmLinear",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneGrid = grid,
tuneLength = 10)

> svm_Linear_Grid
Support Vector Machines with Linear Kernel 

210 samples
13 predictor
2 classes: '0', '1' 

Pre-processing: centered (13), scaled (13) 
Resampling: Cross-Validated (10 fold, repeated 3 times) 
Summary of sample sizes: 189, 189, 189, 189, 189, 189, ... 
Resampling results across tuning parameters:

C     Accuracy   Kappa    
0.00        NaN        NaN
0.01  0.8222222  0.6412577
0.05  0.8285714  0.6540706
0.10  0.8190476  0.6349189
0.25  0.8174603  0.6324448
0.50  0.8126984  0.6232932
0.75  0.8142857  0.6262578
1.00  0.8158730  0.6294200
1.25  0.8158730  0.6294200
1.50  0.8158730  0.6294200
1.75  0.8126984  0.6230572
2.00  0.8126984  0.6230572
5.00  0.8126984  0.6230572

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was C = 0.05. 
> plot(svm_Linear_Grid)

> test_pred_grid <- predict(svm_Linear_Grid, newdata = testing)
> test_pred_grid
[1] 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0 1 0
[45] 0 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0
[89] 1 0
Levels: 0 1

> confusionMatrix(test_pred_grid, testing$V14 )
Confusion Matrix and Statistics

Reference
Prediction  0  1
0 46  5
1  6 33

Accuracy : 0.8778          
95% CI : (0.7918, 0.9374)
No Information Rate : 0.5778          
P-Value [Acc > NIR] : 5.854e-10       

Kappa : 0.7504          
Mcnemar's Test P-Value : 1               

Sensitivity : 0.8846          
Specificity : 0.8684          
Pos Pred Value : 0.9020          
Neg Pred Value : 0.8462          
Prevalence : 0.5778          
Detection Rate : 0.5111          
Detection Prevalence : 0.5667          
Balanced Accuracy : 0.8765          

'Positive' Class : 0 


```

